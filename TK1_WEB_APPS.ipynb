{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSzZZOhvfldK",
        "outputId": "46c55be9-8d4e-4d4f-db01-d8f9fe71d928"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFDHEShOfnXy"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import math\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uYa9_thfo7J"
      },
      "outputs": [],
      "source": [
        "# A. Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,;!?]', '', text.lower())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxoBNkbqfr8w"
      },
      "outputs": [],
      "source": [
        "# B. Tokenization\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypuz9ySUftnO"
      },
      "outputs": [],
      "source": [
        "# C. Remove stop words\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XH56BqOfvdG"
      },
      "outputs": [],
      "source": [
        "# D. Perform lemmatization\n",
        "def perform_lemmatization(tokens):\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTvHCxTefwmq"
      },
      "outputs": [],
      "source": [
        "# E. Build n-gram model and calculate probabilities\n",
        "def build_ngram_model(tokens, n):\n",
        "    ngrams_list = ngrams(tokens, n)\n",
        "    ngram_counts = Counter(ngrams_list)\n",
        "    total_ngrams = sum(ngram_counts.values())\n",
        "    ngram_probabilities = {ngram: count / total_ngrams for ngram, count in ngram_counts.items()}\n",
        "    return ngram_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn_B1uc_fzNy"
      },
      "outputs": [],
      "source": [
        "# F. Simple probability for prediction\n",
        "def simple_prob_predict_next_word(input_text, ngram_probabilities, n):\n",
        "    tokens = preprocess_text(input_text)\n",
        "    tokens = tokenize(tokens)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = perform_lemmatization(tokens)\n",
        "    ngram_prefix = tuple(tokens[-(n-1):])\n",
        "\n",
        "    predictions = []\n",
        "    for ngram, prob in ngram_probabilities.items():\n",
        "        if ngram[:n-1] == ngram_prefix:\n",
        "            predictions.append((ngram[-1], prob))\n",
        "\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    return predictions\n",
        "\n",
        "# G. Bayesian prediction\n",
        "def bayesian_predict_next_word(input_text, ngram_probabilities, n):\n",
        "    tokens = preprocess_text(input_text)\n",
        "    tokens = tokenize(tokens)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = perform_lemmatization(tokens)\n",
        "    ngram_prefix = tuple(tokens[-(n-1):])\n",
        "\n",
        "    # Filter n-grams that match the prefix\n",
        "    relevant_ngrams = {ngram: prob for ngram, prob in ngram_probabilities.items() if ngram[:n-1] == ngram_prefix}\n",
        "\n",
        "    # Calculate the denominator for Bayesian probability\n",
        "    denominator = sum(relevant_ngrams.values())\n",
        "\n",
        "    # Calculate Bayesian probabilities\n",
        "    predictions = [(ngram[-1], (prob + 1) / (denominator + len(ngram_probabilities))) for ngram, prob in relevant_ngrams.items()]\n",
        "\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKqpvv4Hf2f-"
      },
      "outputs": [],
      "source": [
        "# I. Fungsi untuk mendapatkan hasil berupa teks dengan kata-kata saja\n",
        "def get_formatted_predictions(predictions, max_words):\n",
        "    # Ambil hanya sejumlah maksimum kata yang diinginkan\n",
        "    predictions = predictions[:max_words]\n",
        "\n",
        "    # Ambil kata-kata saja\n",
        "    formatted_predictions = [word for word, prob in predictions]\n",
        "\n",
        "    # Gabungkan hasil menjadi kalimat\n",
        "    result = ' '.join(formatted_predictions)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhFgjfSi9c-m"
      },
      "source": [
        "## **Buat di uji coba**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En2nd2HQf6bJ"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the text from the file\n",
        "file_path = 'food.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "# Preprocess the text and tokenize\n",
        "preprocessed_text = preprocess_text(text_data)\n",
        "tokens = tokenize(preprocessed_text)\n",
        "tokens = remove_stopwords(tokens)\n",
        "tokens = perform_lemmatization(tokens)\n",
        "\n",
        "# Build n-gram model (here we use trigrams, n=3)\n",
        "ngram_probabilities = build_ngram_model(tokens, n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGuWOF1xXd1k"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Simpan objek n-gram probabilitas ke dalam file menggunakan pickle\n",
        "with open('ngram_probabilities.pkl', 'wb') as f:\n",
        "    pickle.dump(ngram_probabilities, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4X9cwY-f_28",
        "outputId": "a9fd7d92-1f0a-4a67-b013-3f5119fba5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Simple Probability Predictions:\n",
            "my parent told me to eat one couple smaller tart gluten\n",
            "\n",
            "Bayesian Predictions:\n",
            "my parent told me to eat one couple smaller tart gluten\n"
          ]
        }
      ],
      "source": [
        "# Input text for prediction\n",
        "input_text = \"my parent told me to eat\"\n",
        "# Set the maximum number of words to display in predictions\n",
        "max_words = 5\n",
        "\n",
        "# Perform predictions using simple probability\n",
        "simple_prob_predictions = simple_prob_predict_next_word(input_text, ngram_probabilities, n=3)\n",
        "simple_prob_output = get_formatted_predictions(simple_prob_predictions, max_words)\n",
        "\n",
        "# Perform predictions using Bayesian approach\n",
        "bayesian_predictions = bayesian_predict_next_word(input_text, ngram_probabilities, n=3)\n",
        "bayesian_output = get_formatted_predictions(bayesian_predictions, max_words)\n",
        "\n",
        "print(\"\\nSimple Probability Predictions:\")\n",
        "print(input_text, simple_prob_output)\n",
        "\n",
        "print(\"\\nBayesian Predictions:\")\n",
        "print(input_text, bayesian_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "976f0HdLDM_G",
        "outputId": "c0a6ccb3-010c-4cfb-e730-98e00804b42f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Simple Probability Predictions:\n",
            "one: 1.4487970493219774e-07\n",
            "couple: 7.243985246609887e-08\n",
            "smaller: 7.243985246609887e-08\n",
            "tart: 7.243985246609887e-08\n",
            "gluten: 7.243985246609887e-08\n",
            "\n",
            "Bayesian Predictions:\n",
            "one: 9.324113261848446e-08\n",
            "couple: 9.324112586411156e-08\n",
            "smaller: 9.324112586411156e-08\n",
            "tart: 9.324112586411156e-08\n",
            "gluten: 9.324112586411156e-08\n"
          ]
        }
      ],
      "source": [
        "# Print the predictions\n",
        "print(\"\\nSimple Probability Predictions:\")\n",
        "print(\"\\n\".join([f\"{word}: {prob}\" for word, prob in simple_prob_predictions[:max_words]]))\n",
        "\n",
        "print(\"\\nBayesian Predictions:\")\n",
        "print(\"\\n\".join([f\"{word}: {prob}\" for word, prob in bayesian_predictions[:max_words]]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
